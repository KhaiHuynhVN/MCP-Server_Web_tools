# ðŸŒ WEB MCP SERVER - CONTEXT DOCUMENTATION
## Comprehensive Technical Analysis

# ========================================
# ðŸ“‹ PROJECT OVERVIEW
# ========================================

## Tá»•ng quan vá» Web MCP Server
Web MCP Server lÃ  má»™t Model Context Protocol (MCP) server Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho viá»‡c duyá»‡t web vÃ  thu tháº­p thÃ´ng tin tá»« internet. Server cung cáº¥p 2 tools chÃ­nh Ä‘á»ƒ há»— trá»£ AI agent trong viá»‡c:
- TÃ¬m kiáº¿m thÃ´ng tin trÃªn Google (web_search_tool)
- Láº¥y vÃ  trÃ­ch xuáº¥t ná»™i dung tá»« URLs (web_fetch_tool)

## Project Structure
```
Web_mcp/
â”œâ”€â”€ mcp_server.py              # Main MCP server entry point
â”œâ”€â”€ context.txt                # Context documentation (this file)
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ pyproject.toml             # Project dependencies & config
â”œâ”€â”€ INSTALL_DEPENDENCIES.md    # Detailed installation guide
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ mcp_web_search/        # Web Search Tool
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Module exports
â”‚   â”‚   â”œâ”€â”€ core.py            # Main tool implementation
â”‚   â”‚   â”œâ”€â”€ constants.py       # Configuration & settings
â”‚   â”‚   â””â”€â”€ google_api.py      # Google Custom Search API client
â”‚   â””â”€â”€ mcp_web_fetch/         # Web Fetch Tool  
â”‚       â”œâ”€â”€ __init__.py        # Module exports
â”‚       â”œâ”€â”€ core.py            # Main tool implementation
â”‚       â”œâ”€â”€ constants.py       # Configuration & settings
â”‚       â””â”€â”€ fetch_api.py       # HTTP client & content extraction
â””â”€â”€ venv/                      # Virtual environment
```

# ========================================
# ðŸ” WEB SEARCH TOOL (mcp_web_search_tool)
# ========================================

## Chá»©c nÄƒng chÃ­nh
**Tool Description**: "Web Search Tool - Tim kiem Google va tra ve danh sach URLs"

### Core Features:
- TÃ¬m kiáº¿m trÃªn Google vá»›i tá»« khÃ³a Ä‘Æ°á»£c cung cáº¥p
- Tráº£ vá» ~15 URLs Ä‘áº§u tiÃªn kÃ¨m theo title, snippet vÃ  source
- Hoáº¡t Ä‘á»™ng nhÆ° Google Search thÃ´ng thÆ°á»ng  
- Há»— trá»£ nhiá»u ngÃ´n ngá»¯ tÃ¬m kiáº¿m

### Parameters:
```python
{
    "query": {
        "type": "string",
        "required": True,
        "description": "Tá»« khÃ³a tÃ¬m kiáº¿m (báº¯t buá»™c)"
    },
    "language": {
        "type": "string", 
        "default": "en",
        "description": "NgÃ´n ngá»¯ tÃ¬m kiáº¿m: 'en', 'vi', 'ja', etc."
    }
}
```

### Output Format:
```json
{
    "query": "tá»« khÃ³a tÃ¬m kiáº¿m",
    "total_results": 15,
    "results": [
        {
            "rank": 1,
            "title": "TiÃªu Ä‘á» trang web",
            "url": "https://example.com/page", 
            "snippet": "MÃ´ táº£ ngáº¯n gá»n vá» ná»™i dung trang...",
            "source": "example.com"
        }
    ],
    "status": "success"
}
```

## Technical Implementation

### Google Custom Search API Configuration
- File: `tools/mcp_web_search/constants.py`
- API Key: AIzaSyB9vTfPHy0eGZHenrynIpBy9TfbL6vxRFA
- Search Engine ID: d0d71d459e6a8428e
- Default results: 15 URLs
- Max results per request: 10 (Google API limit)
- Max API requests: 5

### Core Implementation
- File: `tools/mcp_web_search/core.py`
- Main function: `web_search_tool(query: str, language: str = "en") -> str`
- Returns: JSON string with search results
- Error handling: Graceful fallback vá»›i meaningful error messages

### Google API Client
- File: `tools/mcp_web_search/google_api.py` 
- Class: `GoogleCustomSearchAPI`
- Supports: Multiple requests for results > 10
- Features: Domain extraction, error handling, language support

## Use Cases:
- TÃ¬m kiáº¿m thÃ´ng tin, bÃ i viáº¿t, tutorials
- Research vá» topics cá»¥ thá»ƒ
- TÃ¬m documentation, coding guides  
- Láº¥y danh sÃ¡ch sources Ä‘á»ƒ Ä‘á»c thÃªm

# ========================================
# ðŸ“„ WEB FETCH TOOL (mcp_web_fetch_tool)  
# ========================================

## Chá»©c nÄƒng chÃ­nh
**Tool Description**: "Web Fetch Tool - Fetch vÃ  extract content tá»« URLs"

### Core Features:
- Fetch content tá»« má»™t hoáº·c nhiá»u URLs
- Extract readable text, title, description tá»« HTML
- Support multiple content types (HTML, JSON, plain text)
- Extract links tá»« pages (optional)
- Error handling robust cho failed requests

### Parameters:
```python
{
    "url": {
        "type": ["string", "array"],
        "required": True,
        "description": "URL hoáº·c danh sÃ¡ch URLs Ä‘á»ƒ fetch content. Max 50 URLs per request"
    },
    "extract_links": {
        "type": "boolean",
        "default": True,
        "description": "CÃ³ extract links tá»« pages khÃ´ng"
    }
}
```

### Output Format:
```json
{
    "total_urls": 1,
    "successful_fetches": 1,
    "results": [
        {
            "url": "https://example.com",
            "title": "Page Title",
            "description": "Meta description...",
            "content": "Main text content...",
            "content_type": "text/html",
            "word_count": 250,
            "links": [...],
            "status": "success",
            "javascript_rendered": false
        }
    ],
    "status": "success",
    "message": "ThÃ nh cÃ´ng fetch 1/1 URLs. Tá»•ng cá»™ng 250 tá»«."
}
```

## Advanced Technical Implementation

### HTTP Client Configuration
- File: `tools/mcp_web_fetch/constants.py`
- HTTP/2 Support: httpx client with fallback to requests
- Request timeout: 300 seconds (5 minutes for massive docs)
- Max content size: 500MB 
- Max redirects: 20
- Smart retry: 5 attempts with multiple strategies

### User-Agent Pool (Anti-Detection)
- Modern browser pool: Chrome 131, Firefox 133, Safari 18, Edge 131
- Complete browser profiles with matching headers
- Random rotation for anti-detection
- sec-ch-ua headers for Chrome compatibility

### Content Extraction (Research-Backed)
**Primary Method**: Trafilatura (90.9% F1-score research-backed)
- `trafilatura.html2txt()` - Extract ALL content including navigation
- `trafilatura.extract()` vá»›i favor_recall=True
- Maximizes content recall for comprehensive extraction

**Fallback Methods**:
1. **readability-lxml** (Mozilla algorithm, 80.1% F1-score)
2. **Smart BeautifulSoup** with minimal removal
3. **Semantic selectors**: main, article, [role="main"], .content

### JavaScript Rendering Architecture
**Primary**: Playwright with stealth mode
- Modern browser automation
- Anti-detection capabilities  
- Real browser fingerprints
- **Enhanced dynamic content waiting**:
  - Framework-aware waiting (React detection)
  - Multi-stage scrolling for lazy-loaded content
  - Extended timeouts for complex SPAs

**Fallback**: requests-html (Legacy support)
- Lightweight PyJS environment
- Basic JavaScript execution

**ALWAYS JS RENDER Approach**:
- **ALL HTML sites** are processed with Playwright rendering
- **Maximum reliability** - never misses JS-rendered content
- **Consistent behavior** for static and dynamic sites
- **Future-proof** with any JS framework

**CRITICAL FIX - Links Extraction Order**:
- **Links extracted FIRST** from fully rendered DOM
- **Before content extraction libraries** clean/modify HTML  
- **Ensures JS-rendered links** are captured correctly

### Content Type Support
- HTML content: Full extraction with JS rendering capability
- JSON: Formatted JSON pretty print
- Plain text: Direct text extraction
- Flexible content-type matching for maximum compatibility

## Core Implementation Files

### Main Tool Interface
- File: `tools/mcp_web_fetch/core.py`
- Main function: `web_fetch_tool(url: Union[str, List[str]], extract_links: bool = True) -> str`
- Batch processing: Up to 50 URLs per request
- Error handling: Graceful degradation with detailed error messages

### HTTP Client & Content Extraction
- File: `tools/mcp_web_fetch/fetch_api.py` 
- Class: `WebContentFetcher`
- Features:
  - HTTP/2 primary, HTTP/1.1 fallback
  - Domain-specific session management
  - Smart retry with multiple strategies
  - Charset detection with charset-normalizer
  - JavaScript rendering with Playwright/requests-html

### Configuration & Constants
- File: `tools/mcp_web_fetch/constants.py`
- Browser profiles: Complete headers for Chrome, Firefox, Safari, Edge
- Retry strategies: exponential_backoff, linear_progression, fibonacci_sequence
- JavaScript detection patterns
- Content extraction limits
- **MAX_LINKS_EXTRACT = 300**: Configurable maximum links per page (increased from 100 for documentation sites)

## Use Cases:
- Láº¥y ná»™i dung chi tiáº¿t tá»« URLs tÃ¬m Ä‘Æ°á»£c qua web_search_tool
- Extract text content tá»« articles, blogs, documentation  
- Batch processing multiple URLs cÃ¹ng lÃºc
- Content analysis vÃ  research

# ========================================
# ðŸ”§ TECHNICAL ARCHITECTURE
# ========================================

## MCP Server Configuration
- File: `mcp_server.py`
- Framework: FastMCP (Model Context Protocol)
- Transport: stdio
- Tools registered: web_search_tool, web_fetch_tool

## Dependencies & Installation
- Python requirement: >=3.13
- Core dependencies: mcp, requests, beautifulsoup4, lxml
- HTTP/2 support: httpx[http2], h2
- JS rendering: playwright, playwright-stealth, requests-html
- Content extraction: trafilatura, readability-lxml
- Charset detection: charset-normalizer

### Installation Options:
```bash
# Quick install from project directory
cd Web_mcp
pip install -e .

# Manual dependency install
pip install mcp requests httpx[http2] h2 playwright playwright-stealth requests-html beautifulsoup4 lxml trafilatura readability-lxml charset-normalizer

# Minimal install (basic features only)
pip install mcp requests beautifulsoup4 lxml
```

## Performance & Reliability Features

### Smart Retry System
- Multiple retry strategies with intelligent delay calculation
- Error classification for retriable vs non-retriable errors
- Browser profile rotation for enhanced anti-detection
- Session and cookie management per domain

### Memory & Performance Optimization
- Content length limits: 2M characters for massive technical docs
- Batch processing: Up to 50 URLs per request  
- Connection pooling: Domain-specific sessions
- Intelligent approach selection: Static vs JS rendering

### Anti-Detection Features
- Real browser user-agent pools (current market share based)
- Complete browser profiles with matching headers
- Cookie persistence per domain
- HTTP/2 support for modern site compatibility

# ========================================
# ðŸ’¼ WORKFLOW INTEGRATION
# ========================================

## Typical Usage Pattern:
1. **Search Phase**: Use `web_search_tool` to find relevant URLs
   ```python
   search_results = web_search_tool("CSS accessibility guidelines")
   ```

2. **Content Extraction Phase**: Use `web_fetch_tool` to get detailed content
   ```python
   urls = [result["url"] for result in search_results["results"][:5]]
   content = web_fetch_tool(urls, extract_links=True)
   ```

## Integration with AI Agents:
- **Research Tasks**: Search â†’ Fetch â†’ Analyze pattern
- **Documentation Gathering**: Batch URL processing
- **Content Analysis**: Deep content extraction with link mapping
- **Multi-source Research**: Combine search results with detailed content

## Error Handling Strategy:
- **Graceful degradation**: Tool continues working even with partial failures
- **ðŸ”¥ ERROR COMMUNICATION**: Errors returned via tool response instead of logging
- **Structured error responses**: All errors include type, details, and timestamps
- **Fallback mechanisms**: HTTP/2 â†’ HTTP/1.1, Playwright â†’ static extraction
- **Retry logic**: Smart retry with detailed retry history in responses
- **Always JS First**: Maximum reliability approach ensures best results
- **ðŸ”¥ CRASH-PROOF Links**: Robust error handling prevents tool crashes
- **Never Silent Failures**: All errors communicated via JSON response

# ========================================
# ðŸŽ¯ USAGE GUIDELINES & TIPS
# ========================================

## For Web Search Tool:
- Use specific, descriptive search queries
- Leverage language parameter for non-English content
- Combine multiple searches for comprehensive coverage
- Use search results as input for web_fetch_tool

## For Web Fetch Tool:
- **Always gets complete content** - now uses Playwright for all HTML sites
- Batch URLs from same domain for better performance
- Use extract_links=True for discovering related content  
- Monitor word_count in results for content richness
- **100% reliability** - never misses JS-rendered content

## Performance Optimization:
- Process URLs in batches rather than individually
- Use extract_links=False if only need main content
- Monitor successful_fetches ratio for reliability
- Leverage domain-specific session caching

## Content Quality Tips:
- Prioritize sites with high text-to-HTML ratios
- Use JavaScript rendering for SPA frameworks
- Combine with search tool for comprehensive research
- Monitor content_type for data format planning

# ========================================
# ðŸš€ FUTURE ENHANCEMENTS
# ========================================

## Planned Features:
- Additional search engines (Bing, DuckDuckGo)
- PDF content extraction
- Image text extraction (OCR)
- Advanced content filtering
- Caching layer for frequently accessed URLs

## Current Status:
- âœ… Web Search Tool: COMPLETED & PRODUCTION READY
- âœ… Web Fetch Tool: COMPLETED & PRODUCTION READY 
- ðŸš€ **RELIABILITY ENHANCED**: Always JS Render approach implemented
- ðŸŽ¯ **100% Content Capture**: Never misses JS-rendered content
- ðŸ”„ Enhanced search engines: PLANNED
- ðŸ”„ Advanced content types: PLANNED

===== END OF CONTEXT =====
