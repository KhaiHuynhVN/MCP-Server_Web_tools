# 🌐 WEB MCP SERVER - CONTEXT DOCUMENTATION
## Comprehensive Technical Analysis

# ========================================
# 📋 PROJECT OVERVIEW
# ========================================

## Tổng quan về Web MCP Server
Web MCP Server là một Model Context Protocol (MCP) server được thiết kế đặc biệt cho việc duyệt web và thu thập thông tin từ internet. Server cung cấp 2 tools chính để hỗ trợ AI agent trong việc:
- Tìm kiếm thông tin trên Google (web_search_tool)
- Lấy và trích xuất nội dung từ URLs (web_fetch_tool)

## Project Structure
```
Web_mcp/
├── mcp_server.py              # Main MCP server entry point
├── context.txt                # Context documentation (this file)
├── README.md                  # Project documentation
├── pyproject.toml             # Project dependencies & config
├── INSTALL_DEPENDENCIES.md    # Detailed installation guide
├── tools/
│   ├── mcp_web_search/        # Web Search Tool
│   │   ├── __init__.py        # Module exports
│   │   ├── core.py            # Main tool implementation
│   │   ├── constants.py       # Configuration & settings
│   │   └── google_api.py      # Google Custom Search API client
│   └── mcp_web_fetch/         # Web Fetch Tool  
│       ├── __init__.py        # Module exports
│       ├── core.py            # Main tool implementation
│       ├── constants.py       # Configuration & settings
│       └── fetch_api.py       # HTTP client & content extraction
└── venv/                      # Virtual environment
```

# ========================================
# 🔍 WEB SEARCH TOOL (mcp_web_search_tool)
# ========================================

## Chức năng chính
**Tool Description**: "Web Search Tool - Tim kiem Google va tra ve danh sach URLs"

### Core Features:
- Tìm kiếm trên Google với từ khóa được cung cấp
- Trả về ~15 URLs đầu tiên kèm theo title, snippet và source
- Hoạt động như Google Search thông thường  
- Hỗ trợ nhiều ngôn ngữ tìm kiếm

### Parameters:
```python
{
    "query": {
        "type": "string",
        "required": True,
        "description": "Từ khóa tìm kiếm (bắt buộc)"
    },
    "language": {
        "type": "string", 
        "default": "en",
        "description": "Ngôn ngữ tìm kiếm: 'en', 'vi', 'ja', etc."
    }
}
```

### Output Format:
```json
{
    "query": "từ khóa tìm kiếm",
    "total_results": 15,
    "results": [
        {
            "rank": 1,
            "title": "Tiêu đề trang web",
            "url": "https://example.com/page", 
            "snippet": "Mô tả ngắn gọn về nội dung trang...",
            "source": "example.com"
        }
    ],
    "status": "success"
}
```

## Technical Implementation

### Google Custom Search API Configuration
- File: `tools/mcp_web_search/constants.py`
- API Key: AIzaSyB9vTfPHy0eGZHenrynIpBy9TfbL6vxRFA
- Search Engine ID: d0d71d459e6a8428e
- Default results: 15 URLs
- Max results per request: 10 (Google API limit)
- Max API requests: 5

### Core Implementation
- File: `tools/mcp_web_search/core.py`
- Main function: `web_search_tool(query: str, language: str = "en") -> str`
- Returns: JSON string with search results
- Error handling: Graceful fallback với meaningful error messages

### Google API Client
- File: `tools/mcp_web_search/google_api.py` 
- Class: `GoogleCustomSearchAPI`
- Supports: Multiple requests for results > 10
- Features: Domain extraction, error handling, language support

## Use Cases:
- Tìm kiếm thông tin, bài viết, tutorials
- Research về topics cụ thể
- Tìm documentation, coding guides  
- Lấy danh sách sources để đọc thêm

# ========================================
# 📄 WEB FETCH TOOL (mcp_web_fetch_tool)  
# ========================================

## Chức năng chính
**Tool Description**: "Web Fetch Tool - Fetch và extract content từ URLs"

### Core Features:
- Fetch content từ một hoặc nhiều URLs
- Extract readable text, title, description từ HTML
- Support multiple content types (HTML, JSON, plain text)
- Extract links từ pages (optional)
- Error handling robust cho failed requests

### Parameters:
```python
{
    "url": {
        "type": ["string", "array"],
        "required": True,
        "description": "URL hoặc danh sách URLs để fetch content. Max 50 URLs per request"
    },
    "extract_links": {
        "type": "boolean",
        "default": True,
        "description": "Có extract links từ pages không"
    }
}
```

### Output Format:
```json
{
    "total_urls": 1,
    "successful_fetches": 1,
    "results": [
        {
            "url": "https://example.com",
            "title": "Page Title",
            "description": "Meta description...",
            "content": "Main text content...",
            "content_type": "text/html",
            "word_count": 250,
            "links": [...],
            "status": "success",
            "javascript_rendered": false
        }
    ],
    "status": "success",
    "message": "Thành công fetch 1/1 URLs. Tổng cộng 250 từ."
}
```

## Advanced Technical Implementation

### HTTP Client Configuration
- File: `tools/mcp_web_fetch/constants.py`
- HTTP/2 Support: httpx client with fallback to requests
- Request timeout: 300 seconds (5 minutes for massive docs)
- Max content size: 500MB 
- Max redirects: 20
- Smart retry: 5 attempts with multiple strategies

### User-Agent Pool (Anti-Detection)
- Modern browser pool: Chrome 131, Firefox 133, Safari 18, Edge 131
- Complete browser profiles with matching headers
- Random rotation for anti-detection
- sec-ch-ua headers for Chrome compatibility

### Content Extraction (Research-Backed)
**Primary Method**: Trafilatura (90.9% F1-score research-backed)
- `trafilatura.html2txt()` - Extract ALL content including navigation
- `trafilatura.extract()` với favor_recall=True
- Maximizes content recall for comprehensive extraction

**Fallback Methods**:
1. **readability-lxml** (Mozilla algorithm, 80.1% F1-score)
2. **Smart BeautifulSoup** with minimal removal
3. **Semantic selectors**: main, article, [role="main"], .content

### JavaScript Rendering Architecture
**Primary**: Playwright with stealth mode
- Modern browser automation
- Anti-detection capabilities  
- Real browser fingerprints
- **Enhanced dynamic content waiting**:
  - Framework-aware waiting (React detection)
  - Multi-stage scrolling for lazy-loaded content
  - Extended timeouts for complex SPAs

**Fallback**: requests-html (Legacy support)
- Lightweight PyJS environment
- Basic JavaScript execution

**ALWAYS JS RENDER Approach**:
- **ALL HTML sites** are processed with Playwright rendering
- **Maximum reliability** - never misses JS-rendered content
- **Consistent behavior** for static and dynamic sites
- **Future-proof** with any JS framework

**CRITICAL FIX - Links Extraction Order**:
- **Links extracted FIRST** from fully rendered DOM
- **Before content extraction libraries** clean/modify HTML  
- **Ensures JS-rendered links** are captured correctly

### Content Type Support
- HTML content: Full extraction with JS rendering capability
- JSON: Formatted JSON pretty print
- Plain text: Direct text extraction
- Flexible content-type matching for maximum compatibility

## Core Implementation Files

### Main Tool Interface
- File: `tools/mcp_web_fetch/core.py`
- Main function: `web_fetch_tool(url: Union[str, List[str]], extract_links: bool = True) -> str`
- Batch processing: Up to 50 URLs per request
- Error handling: Graceful degradation with detailed error messages

### HTTP Client & Content Extraction
- File: `tools/mcp_web_fetch/fetch_api.py` 
- Class: `WebContentFetcher`
- Features:
  - HTTP/2 primary, HTTP/1.1 fallback
  - Domain-specific session management
  - Smart retry with multiple strategies
  - Charset detection with charset-normalizer
  - JavaScript rendering with Playwright/requests-html

### Configuration & Constants
- File: `tools/mcp_web_fetch/constants.py`
- Browser profiles: Complete headers for Chrome, Firefox, Safari, Edge
- Retry strategies: exponential_backoff, linear_progression, fibonacci_sequence
- JavaScript detection patterns
- Content extraction limits
- **MAX_LINKS_EXTRACT = 300**: Configurable maximum links per page (increased from 100 for documentation sites)

## Use Cases:
- Lấy nội dung chi tiết từ URLs tìm được qua web_search_tool
- Extract text content từ articles, blogs, documentation  
- Batch processing multiple URLs cùng lúc
- Content analysis và research

# ========================================
# 🔧 TECHNICAL ARCHITECTURE
# ========================================

## MCP Server Configuration
- File: `mcp_server.py`
- Framework: FastMCP (Model Context Protocol)
- Transport: stdio
- Tools registered: web_search_tool, web_fetch_tool

## Dependencies & Installation
- Python requirement: >=3.13
- Core dependencies: mcp, requests, beautifulsoup4, lxml
- HTTP/2 support: httpx[http2], h2
- JS rendering: playwright, playwright-stealth, requests-html
- Content extraction: trafilatura, readability-lxml
- Charset detection: charset-normalizer

### Installation Options:
```bash
# Quick install from project directory
cd Web_mcp
pip install -e .

# Manual dependency install
pip install mcp requests httpx[http2] h2 playwright playwright-stealth requests-html beautifulsoup4 lxml trafilatura readability-lxml charset-normalizer

# Minimal install (basic features only)
pip install mcp requests beautifulsoup4 lxml
```

## Performance & Reliability Features

### Smart Retry System
- Multiple retry strategies with intelligent delay calculation
- Error classification for retriable vs non-retriable errors
- Browser profile rotation for enhanced anti-detection
- Session and cookie management per domain

### Memory & Performance Optimization
- Content length limits: 2M characters for massive technical docs
- Batch processing: Up to 50 URLs per request  
- Connection pooling: Domain-specific sessions
- Intelligent approach selection: Static vs JS rendering

### Anti-Detection Features
- Real browser user-agent pools (current market share based)
- Complete browser profiles with matching headers
- Cookie persistence per domain
- HTTP/2 support for modern site compatibility

# ========================================
# 💼 WORKFLOW INTEGRATION
# ========================================

## Typical Usage Pattern:
1. **Search Phase**: Use `web_search_tool` to find relevant URLs
   ```python
   search_results = web_search_tool("CSS accessibility guidelines")
   ```

2. **Content Extraction Phase**: Use `web_fetch_tool` to get detailed content
   ```python
   urls = [result["url"] for result in search_results["results"][:5]]
   content = web_fetch_tool(urls, extract_links=True)
   ```

## Integration with AI Agents:
- **Research Tasks**: Search → Fetch → Analyze pattern
- **Documentation Gathering**: Batch URL processing
- **Content Analysis**: Deep content extraction with link mapping
- **Multi-source Research**: Combine search results with detailed content

## Error Handling Strategy:
- **Graceful degradation**: Tool continues working even with partial failures
- **🔥 ERROR COMMUNICATION**: Errors returned via tool response instead of logging
- **Structured error responses**: All errors include type, details, and timestamps
- **Fallback mechanisms**: HTTP/2 → HTTP/1.1, Playwright → static extraction
- **Retry logic**: Smart retry with detailed retry history in responses
- **Always JS First**: Maximum reliability approach ensures best results
- **🔥 CRASH-PROOF Links**: Robust error handling prevents tool crashes
- **Never Silent Failures**: All errors communicated via JSON response

# ========================================
# 🎯 USAGE GUIDELINES & TIPS
# ========================================

## For Web Search Tool:
- Use specific, descriptive search queries
- Leverage language parameter for non-English content
- Combine multiple searches for comprehensive coverage
- Use search results as input for web_fetch_tool

## For Web Fetch Tool:
- **Always gets complete content** - now uses Playwright for all HTML sites
- Batch URLs from same domain for better performance
- Use extract_links=True for discovering related content  
- Monitor word_count in results for content richness
- **100% reliability** - never misses JS-rendered content

## Performance Optimization:
- Process URLs in batches rather than individually
- Use extract_links=False if only need main content
- Monitor successful_fetches ratio for reliability
- Leverage domain-specific session caching

## Content Quality Tips:
- Prioritize sites with high text-to-HTML ratios
- Use JavaScript rendering for SPA frameworks
- Combine with search tool for comprehensive research
- Monitor content_type for data format planning

# ========================================
# 🚀 FUTURE ENHANCEMENTS
# ========================================

## Planned Features:
- Additional search engines (Bing, DuckDuckGo)
- PDF content extraction
- Image text extraction (OCR)
- Advanced content filtering
- Caching layer for frequently accessed URLs

## Current Status:
- ✅ Web Search Tool: COMPLETED & PRODUCTION READY
- ✅ Web Fetch Tool: COMPLETED & PRODUCTION READY 
- 🚀 **RELIABILITY ENHANCED**: Always JS Render approach implemented
- 🎯 **100% Content Capture**: Never misses JS-rendered content
- 🔄 Enhanced search engines: PLANNED
- 🔄 Advanced content types: PLANNED

===== END OF CONTEXT =====
